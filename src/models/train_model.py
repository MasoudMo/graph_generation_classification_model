from dataset import PtbEcgDataset
import click
from torch.nn.functional import binary_cross_entropy
from torch import square
from torch import sum
import torch.optim as optim
from gnn_models import *
import torch
import numpy as np
from sklearn.metrics import roc_auc_score
from torch.utils.data import random_split
from visdom import Visdom
import torch


def generation_classification_loss(generated_graph,
                                   original_graph,
                                   classification_predictions,
                                   classification_labels,
                                   num_nodes,
                                   h_log_std,
                                   h_mean):
    """Computes the overall loss

    Combines the generator and classification loss to find the overall cost. This cost contains the ELBO loss and the
    classification loss.

    Args:
        generated_graph:
            The graph generated by the generator
        original_graph:
            The original graph fed into the generator
        classification_predictions:
            The classifier's prediction
        classification_labels:
            The labels for the classifier
        num_nodes:
            The number of nodes in the graph
        h_log_std:
            The log std computed by the generator
        h_mean:
            The mean computed by the generator
    """
    # Compute the reconstruction loss
    reconstruction_loss = binary_cross_entropy(generated_graph, original_graph)

    # Compute the KL loss
    kl_loss = torch.mean(sum(1 + 2*h_log_std - square(h_mean) - square(exp(h_log_std)), dim=1))

    # Compute the classification loss
    classification_loss = binary_cross_entropy(classification_predictions, classification_labels)

    # Add the classification loss
    cost = reconstruction_loss - kl_loss + 10*classification_loss

    return cost


@click.command()
@click.argument('train_data_dir', type=click.Path(exists=True))
@click.argument('train_label_dir', type=click.Path(exists=True))
@click.argument('history_path', type=str)
def train(train_data_dir, train_label_dir, history_path):

    torch.manual_seed(10)

    dataset = PtbEcgDataset(input_data_csv_file=train_data_dir, input_label_csv_file=train_label_dir)

    train_dataset, val_dataset = random_split(dataset, [422, 100])

    print('Training dataset has {} samples'.format(len(train_dataset)))
    print('Validation dataset has {} samples'.format(len(val_dataset)))

    generator_model = VariationalGraphAutoEncoder(input_dim=6, hidden_dim_1=4, hidden_dim_2=2, num_nodes=15)
    classifier_model = BinaryGraphClassifier(input_dim=6, hidden_dim=4)

    graph_generator_optimizer = optim.Adam(generator_model.parameters(), lr=0.001)
    graph_classifier_optimizer = optim.Adam(classifier_model.parameters(), lr=0.001)

    # Scheduler
    scheduler = torch.optim.lr_scheduler.MultiStepLR(graph_classifier_optimizer, milestones=[20, 100, 200], gamma=0.5)

    vis = Visdom()

    for epoch in range(10000):

        generator_model.train()
        classifier_model.train()

        y_true = list()
        y_pred = list()
        epoch_loss = 0

        for features, label in train_dataset:

            generated_graph = generator_model(torch.ones((15, 15)), features)

            classification_predictions = classifier_model(generated_graph.detach(), features)

            y_true.append(label.numpy().flatten())
            y_pred.append(classification_predictions.detach().numpy().flatten())

            # ELBO Loss
            loss = generation_classification_loss(generated_graph=generated_graph,
                                                  original_graph=torch.ones((15, 15)),
                                                  classification_predictions=classification_predictions,
                                                  classification_labels=label,
                                                  num_nodes=generator_model.num_nodes,
                                                  h_log_std=generator_model.h_log_std,
                                                  h_mean=generator_model.h_mean)

            graph_generator_optimizer.zero_grad()
            graph_classifier_optimizer.zero_grad()

            loss.backward()

            graph_generator_optimizer.step()
            graph_classifier_optimizer.step()

            epoch_loss += loss.detach().item()

        epoch_loss /= len(train_dataset)
        y_true = np.array(y_true).flatten()
        y_pred = np.array(y_pred).flatten()

        print('Training epoch {}, loss {:.4f}'.format(epoch, epoch_loss))

        # Compute the roc_auc accuracy
        acc = roc_auc_score(y_true.reshape((-1,)), y_pred.reshape(-1,))
        print("Training epoch {}, accuracy {:.4f}".format(epoch, acc))

        vis.line(Y=torch.reshape(torch.tensor(epoch_loss), (-1, )), X=torch.reshape(torch.tensor(epoch), (-1, )),
                 update='append', win='tain_loss',
                 opts=dict(title="Train Losses Per Epoch", xlabel="Epoch", ylabel="Loss"))

        vis.line(Y=torch.reshape(torch.tensor(acc), (-1, )), X=torch.reshape(torch.tensor(epoch), (-1, )),
                 update='append', win='train_acc',
                 opts=dict(title="Train Accuracy Per Epoch", xlabel="Epoch", ylabel="Accuracy"))

        if history_path:
            f = open(history_path+"_train_losses.txt", "a")
            f.write(str(epoch_loss) + "\n")
            f.close()

            f = open(history_path + "_train_accs.txt", "a")
            f.write(str(acc) + "\n")
            f.close()

        with torch.no_grad():
            y_true = list()
            y_pred = list()
            epoch_loss = 0

        for features, label in val_dataset:

            generated_graph = generator_model(adj=torch.ones((15, 15)), features=features)

            classification_predictions = classifier_model(generated_graph.detach(), features)

            y_true.append(label.numpy().flatten())
            y_pred.append(classification_predictions.detach().numpy().flatten())

            # ELBO Loss
            loss = generation_classification_loss(generated_graph=generated_graph,
                                                  original_graph=torch.ones((15, 15)),
                                                  classification_predictions=classification_predictions,
                                                  classification_labels=label,
                                                  num_nodes=generator_model.num_nodes,
                                                  h_log_std=generator_model.h_log_std,
                                                  h_mean=generator_model.h_mean)

            epoch_loss += loss.detach().item()

        epoch_loss /= len(val_dataset)
        y_true = np.array(y_true).flatten()
        y_pred = np.array(y_pred).flatten()

        print('Validation epoch {}, loss {:.4f}'.format(epoch, epoch_loss))

        # Compute the roc_auc accuracy
        acc = roc_auc_score(y_true.reshape((-1,)), y_pred.reshape(-1,))
        print("Validation epoch {}, accuracy {:.4f}".format(epoch, acc))

        vis.line(Y=torch.reshape(torch.tensor(epoch_loss), (-1,)), X=torch.reshape(torch.tensor(epoch), (-1,)),
                 update='append', win='val_loss',
                 opts=dict(title="Validation Losses Per Epoch", xlabel="Epoch", ylabel="Loss"))

        vis.line(Y=torch.reshape(torch.tensor(acc), (-1,)), X=torch.reshape(torch.tensor(epoch), (-1,)),
                 update='append', win='val_acc',
                 opts=dict(title="Validation Accuracy Per Epoch", xlabel="Epoch", ylabel="Accuracy"))

        if history_path:
            f = open(history_path+"_val_losses.txt", "a")
            f.write(str(epoch_loss) + "\n")
            f.close()

            f = open(history_path + "_val_accs.txt", "a")
            f.write(str(acc) + "\n")
            f.close()

        scheduler.step()


if __name__ == "__main__":
    train()
